[
  {
    "objectID": "virtual-resume.html",
    "href": "virtual-resume.html",
    "title": "Virtual Resume",
    "section": "",
    "text": "My first career experience after finishing my undergraduate degree. During this time, I provided analytical insights and financial reports to multiple clients, all while building a network and learning about professionalism.\nKey Tasks and Accomplishments:\n\nUpdated and monitored financial and health data used in conducting analyses to identify trends, discrepancies, and produce recommendations.\nLeveraged findings during the renewal process and utilized data to negotiate premiums with insurance carriers.\nCommunicated key metrics and statistics by preparing and presenting periodic reports for clients.\nAutomated internal tasks by creating VBA macros and scripts.\n\nSmall scale automation projects during my time as a Benefits Data Analyst helped me start to develop my programming repertoire, and a few of the generic models I built prior to implementing in my work can be found here."
  },
  {
    "objectID": "virtual-resume.html#parker-smith-feek",
    "href": "virtual-resume.html#parker-smith-feek",
    "title": "Virtual Resume",
    "section": "",
    "text": "My first career experience after finishing my undergraduate degree. During this time, I provided analytical insights and financial reports to multiple clients, all while building a network and learning about professionalism.\nKey Tasks and Accomplishments:\n\nUpdated and monitored financial and health data used in conducting analyses to identify trends, discrepancies, and produce recommendations.\nLeveraged findings during the renewal process and utilized data to negotiate premiums with insurance carriers.\nCommunicated key metrics and statistics by preparing and presenting periodic reports for clients.\nAutomated internal tasks by creating VBA macros and scripts.\n\nSmall scale automation projects during my time as a Benefits Data Analyst helped me start to develop my programming repertoire, and a few of the generic models I built prior to implementing in my work can be found here."
  },
  {
    "objectID": "virtual-resume.html#master-of-science-data-science-msds",
    "href": "virtual-resume.html#master-of-science-data-science-msds",
    "title": "Virtual Resume",
    "section": "Master of Science, Data Science (MSDS)",
    "text": "Master of Science, Data Science (MSDS)\n\n\n\n\n\n\n\nUniversity of Colorado. Boulder\n\n2023 - Current\n\n\nExpected Graduation 2025\nKey Accolades\n\nBias in Facial Classification Machine Learning Models"
  },
  {
    "objectID": "virtual-resume.html#bachelor-of-arts-in-mathematics-minor-in-applied-mathematics",
    "href": "virtual-resume.html#bachelor-of-arts-in-mathematics-minor-in-applied-mathematics",
    "title": "Virtual Resume",
    "section": "Bachelor of Arts in Mathematics, Minor in Applied Mathematics",
    "text": "Bachelor of Arts in Mathematics, Minor in Applied Mathematics\n\n\n\n\n\n\n\nUniversity of Washington, Seattle\n\n2013 - 2017\n\n\nGraduated 2017\nKey Accolades\n\nAlpha Sigma Phi Fraternity\nUW Actuary Club\n\nCo-President\nCommunications Chair"
  },
  {
    "objectID": "virtual-resume.html#data-science-nanodegree",
    "href": "virtual-resume.html#data-science-nanodegree",
    "title": "Virtual Resume",
    "section": "Data Science Nanodegree",
    "text": "Data Science Nanodegree\n\n\n\nUdacity\n\n2023\nGained real-world data science experience with curriculum and projects designed by industry experts.\nAn online certificate of completion can be found here.\nKnowledge Gained:\n\nUsed Python across many data science topics, while utilizing Git to document the progress.\nObject Oriented Programming and Web Development.\nETL Pipelines, Natural Language Processing, and Machine Learning Pipelines.\nA/B Testing and Recommendation Engines.\n\nProjects Completed:\n\nCapstone: Natural Language Processing with Articles\nDisaster Response Dashboard\nBasic Flashcards Applications\nEmergency Calls"
  },
  {
    "objectID": "virtual-resume.html#expressway-to-data-science-r-programming-and-tidyverse",
    "href": "virtual-resume.html#expressway-to-data-science-r-programming-and-tidyverse",
    "title": "Virtual Resume",
    "section": "Expressway to Data Science: R Programming and Tidyverse",
    "text": "Expressway to Data Science: R Programming and Tidyverse\n\n\n\nCoursera\n\n2023\nA specialization in R programming earned upon the completion of 3 courses:\n\nIntroduction to R Programming and Tidyverse\nData Analysis with Tidyverse\nR Programming and Tidyverse Capstone Project\n\nAn online certificate of completion can be found here.\nKnowledge Gained:\n\nUse R and the tidyverse to import data from external repositories and local CSV files.\nClean and transform data for analysis, construct visualizations.\nDocument and reflect on analyses using RMarkdown.\n\nProjects Completed:\n\nCapstone: COVID-19 Analysis"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Project Archive",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nDescription\n\n\n\n\n\n\nMay 1, 2024\n\n\nThe Brewery Project\n\n\n\n\n\n\n\nDec 18, 2023\n\n\nBias in Facial Classification ML Models\n\n\n\n\n\n\n\nDec 18, 2023\n\n\nSnowbound Analytics\n\n\n\n\n\n\n\nAug 23, 2023\n\n\nCOVID-19 Analysis\n\n\n\n\n\n\n\nJul 30, 2023\n\n\nNatural Language Processing with Articles\n\n\n\n\n\n\n\nApr 27, 2023\n\n\nDisaster Response Dashboard\n\n\n\n\n\n\n\nMar 28, 2023\n\n\nBasic Data Dashboard\n\n\n\n\n\n\n\nJan 13, 2023\n\n\nBasic Flashcards Application\n\n\n\n\n\n\n\nNov 21, 2022\n\n\nEmergency Calls\n\n\n\n\n\n\n\nJun 16, 2022\n\n\nRudimentary Python Projects\n\n\n\n\n\n\n\nApr 24, 2022\n\n\nVBA Projects\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/featured/brewery-project/index.html",
    "href": "posts/featured/brewery-project/index.html",
    "title": "The Brewery Project",
    "section": "",
    "text": "Utilized data mining techniques to prepare, clean, and apply data scraped and aggregated from multiple sources to ultimately create models important for brewers and brewery customers to make informed decisions.\nGitHub Repository\nProject Website\nMy team, consisting of three members, performed a full data science life cycle process to help multiple facets of the brewery community make informed decisions. After scraping data and using APIs, this data was cleaned, prepared and then modeled. Several different ideas were tested through exploratory data analysis and the modeling. The modeling consisted of hypertuning parameters.\nSee the links above for the complete analysis and documentation."
  },
  {
    "objectID": "posts/all/snowbound/index.html",
    "href": "posts/all/snowbound/index.html",
    "title": "Snowbound Analytics",
    "section": "",
    "text": "Applied the data science life cycle to data surrounding ski resorts; including weather, nearby businesses, and resort ratings across multiple parameters. An additional aspect featuring spatiotemporal analysis is also featured.\nGitHub Repository\nProject Website"
  },
  {
    "objectID": "posts/all/nlp-articles/index.html",
    "href": "posts/all/nlp-articles/index.html",
    "title": "Natural Language Processing with Articles",
    "section": "",
    "text": "Created an NLP model which attempted to predict the publisher given a news article.\nGitHub Repository\nMedium Article\nThe goal of this project was to attempt to create an NLP model which could correctly identify the publisher given an article.\nAfter cleaning and exploring the data via statistical and visual methods, attempt to create a classification model which will accurately predict publisher if given a news article.\nWithout higher processing power, and a more equipt dataset, this will likely not be very accurate. Additionally, this only accounts for a small portion of the thousands of publishers and news networks, which means any article from outside the publishers in this dataset will result in an incorrect prediction.\nOverall, we were able to successfully clean and analyze a dataset surrounding our goal. After testing several models and parameters, there was a “best” model built. The f1-scores and accuracy were not ideal, however, the framework to continue to improve this model has been created!\nWhen it comes to potential improvements of this analysis and model, at the top of the list is obviously creating more accurate predictions. A starting point would be to perform a more robust parameter search. Given the proper hardware, it would be great to test the entire dataset (~150,000 articles) across not only each of the classifiers, but a grid search of parameters for each of the classifiers as well.\nUnfortunately, the instance of testing just the 3 max_depth parameters for the Random Forest Classifier on less than 10,000 rows of data took about 2 hours on the machine it was performed on. Maybe another machine is better equipt to take on the challenge!"
  },
  {
    "objectID": "posts/all/emergency-calls/index.html",
    "href": "posts/all/emergency-calls/index.html",
    "title": "Emergency Calls",
    "section": "",
    "text": "Analysis of Seattle Police Department call center data.\nGitHub Repository\nMedium Article\nWith such a massive amount of well-maintained information available, this provided an opportunity to dive into what goes on when a police assistance call is placed. Specifically, we looked at:\n\nWhat times were the majority of the calls placed? Are there any surges by day of the month, or month of the year?\nDo the amount of calls change by Precinct, Sector or Beat?\nDoes the Priority of calls change by Precinct, Sector or Beat?\nThe Computer Aided Dispatch (CAD) system assigns Priority. Can we create a model which accurately predicts Priority?\n\nPlease see the GitHub repo and Medium articles for complete findings."
  },
  {
    "objectID": "posts/all/covid-analysis/index.html",
    "href": "posts/all/covid-analysis/index.html",
    "title": "COVID-19 Analysis",
    "section": "",
    "text": "Analyzed COVID-19 from several sources using R.\nGitHub Repository\nThe goal of this project was to manipulate COVID-19 and population data across multiple sources to solve desired inquiries. Local data from the United States as well as global data is analyzed.\nAmong the findings, there was one such visual I found compelling. The visual is of California broken down into counties showing a correlation of areas with higher concentrations of cases having higher concentrations of death.\n\n\n\nCalifornia Cases and Deaths"
  },
  {
    "objectID": "posts/all/bias-in-ml-models/index.html",
    "href": "posts/all/bias-in-ml-models/index.html",
    "title": "Bias in Facial Classification ML Models",
    "section": "",
    "text": "An exploratory and statistical analysis on the biases prevalent in facial recognition machine learning models.\nGitHub Repository\nProject Website\nYouTube Video\nMy team, consisting of six members, performed a litany of analyses to test if there was bias in facial recognition machine learning models. We tested the DeepFace and FairFace algorithms against a large and diverse dataset. Performing both the classic performance measurements such as accuracy and f1-score, and categorical hypothesis testing (proportionality testing), we were able to find some instances of bias. Ironically, perhaps our biggest discovery was that categorial hypoothesis testing (proportionality testing) was not a strong indicator to identify issues and errors in machine learning models.\nSee the links above for the complete analysis and documentation."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Carl Klein",
    "section": "",
    "text": "Welcome to my website! When I’m not in the mountains, I’m fine tuning my data science skills.\nI decided to pursue my master’s degree in data science (MSDS) at CU Boulder to apply my passions for mathematics and programming in an interdisciplinary setting which is known for exposure in the Earth and natural sciences, among other fascinating and complex subjects.\nPrior to this, I worked as a Benefits Data Analyst in the insurance industry. Initially, I had actuarial aspirations, inspired by my time at the University of Washington, where I studied mathematics (Bachelor of Arts in Mathematics, minor in Applied Mathematics).\nSmall scale automation projects during my time as a Benefits Data Analyst helped me start to develop a programming repertoire. I kept building the programming toolkit, eventually merging this with my passion for mathematics, and ultimately leading me down the data science pathway.\nDuring my time at CU Boulder, I’ve excelled at applying statistical theory, data mining, and machine learning algorithms; all of which can be viewed throughout my showcased projects and GitHub repositories.\nFeel free to peruse my projects and GitHub repositories. Thanks for visiting, and please don’t hesitate to reach out with any questions!"
  },
  {
    "objectID": "featured_projects.html",
    "href": "featured_projects.html",
    "title": "Featured Projects",
    "section": "",
    "text": "The Brewery Project\n\n\n\n\n\n\npython\n\n\nanalysis\n\n\nexploratory\n\n\nquarto\n\n\nplotly\n\n\nmachine-learning\n\n\ndata-mining\n\n\n\n\n\n\n\n\n\nMay 1, 2024\n\n\nCarl Klein, Jessica Groven, Julia Gallwitz\n\n\n\n\n\n\n\n\n\n\n\n\nSnowbound Analytics\n\n\n\n\n\n\npython\n\n\nanalysis\n\n\nbootstrap\n\n\nflask\n\n\njavascript\n\n\nexploratory\n\n\nquarto\n\n\nplotly\n\n\nmachine-learning\n\n\ndata-mining\n\n\napi\n\n\nhtml\n\n\nspatial-analysis\n\n\ntemporal-analysis\n\n\n\n\n\n\n\n\n\nDec 18, 2023\n\n\nCarl Klein\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "The About Page"
  },
  {
    "objectID": "guides.html",
    "href": "guides.html",
    "title": "Guides",
    "section": "",
    "text": "Featured on this page are links to guides and packages I’ve created across several foundation topics in data science.\n\nData Mining\nThe basics of data mining with Python code examples.\n\n\nStatistics\nBasic to Advanced statistical topics catered towards data science applications with R code examples.\n\n\nSpatial Analytics\nThe basics of spatial analytics with research notes.\n\n\nFlask Blueprints\nA module which can create a Flask web application projects. Contains options for:\n\nbootstrap styling\ndatabase implementation\ndatabase models\n\n\n\nSQL\nThe basics of SQL."
  },
  {
    "objectID": "posts/all/basic-data-dash/index.html",
    "href": "posts/all/basic-data-dash/index.html",
    "title": "Basic Data Dashboard",
    "section": "",
    "text": "Template and instructions for deploying a dashboard which can be uploaded to a webpage using basic HTML, Flask, Plotly, and Bootstrap\nGitHub Repository\nThis template lays the foundation for creating a dashboard that can be uploaded to a webpage using basic HTML, Flask, Plotly, and Bootstrap. In the link above is a highly detailed guide in building and deploying a web application through Heroku. It covers Ubunto (linux) and Windows (GitBash) approaches."
  },
  {
    "objectID": "posts/all/brewery-project/index.html",
    "href": "posts/all/brewery-project/index.html",
    "title": "The Brewery Project",
    "section": "",
    "text": "Utilized data mining techniques to prepare, clean, and apply data scraped and aggregated from multiple sources to ultimately create models important for brewers and brewery customers to make informed decisions.\nGitHub Repository\nProject Website\nMy team, consisting of three members, performed a full data science life cycle process to help multiple facets of the brewery community make informed decisions. After scraping data and using APIs, this data was cleaned, prepared and then modeled. Several different ideas were tested through exploratory data analysis and the modeling. The modeling consisted of hypertuning parameters.\nSee the links above for the complete analysis and documentation."
  },
  {
    "objectID": "posts/all/disaster-response-dash/index.html",
    "href": "posts/all/disaster-response-dash/index.html",
    "title": "Disaster Response Dashboard",
    "section": "",
    "text": "Created a pipeline which classifies messages from various sources during an emergency. Additional step taken in deploying to Flask web application.\nGitHub Repository\nThe Disaster Response Pipeline classifies messages from various sources during an emergency. Rather than searching through potentially important key words, a model has been trained to categorize each message.\nIn the midst of an emergency, such as a natural disaster, thousands of messages are being sent. It’s important to be able to categorize these messages to optimize efforts and resources.\nBy utilizing natural language processing in a pipeline, a model was built to do just this.\nThe project used the following layout:\n\nETL Pipeline\nML Pipeline\nFlask Web App\n\nFuture Considerations:\n\nDue to time constraints and hardware limitations, a more complex model wasn’t tested or constructed. In the future it would be great to expand the parameter list while using GridSearchCV and try several different classifiers. XGBoost would be an ideal candidate.\nResearch for GridSearchCV also provided alternatives for GridSeachCV, itself. Among the list were RandomizedSearchCV and BayesSearch.\nIt should be mentioned that a significant portion of messages actually weren’t sorted into any of the 36 categories. This may initially feel like an error to train a model using these type of data points, however they are actually significant in the fact they are irrelevant. If a model is trained on only relevant messages, it will place a false priority on a irrelevant messages.\n\nUltimately, this project will benefit the community. In the event of a future disaster, millions of communications will be sent out, and response organizations will be at their full capacity. A model like this will help guide these organizations where to best use their resources."
  },
  {
    "objectID": "posts/all/flashcards-basic/index.html",
    "href": "posts/all/flashcards-basic/index.html",
    "title": "Basic Flashcards Application",
    "section": "",
    "text": "Application which allows users to build and combine decks of virtual flashcards. Supports mathematical notation and symbols.\nGitHub Repository\nActually, one of my first completed object oriented programming projects.\nThis project allows a user to create flashcards and decks of flashcards. It is meant to be a virtual replacement to physical flashcards, and features abilities such as deck combination and shuffling.\nIt was imperative to include the SymPy library, as this will personally be used heavily for mathematical concepts. On another personal note, further motivation for developing this into some sort of application was to swap mindless scrolling with learning during downtime."
  },
  {
    "objectID": "posts/all/rud-python-projects/index.html",
    "href": "posts/all/rud-python-projects/index.html",
    "title": "Rudimentary Python Projects",
    "section": "",
    "text": "As I was just starting to practice coding in python. Additionally, added to my portfolio prior to a fuller understanding of Git and writing “proper” READMEs.\nGitHub Repository - Expense Tracker Tool\nGitHub Repository - Linear Algebra Tools\nSome of the first functions written and files imported via Python were within these files. Normally, not something to showcase within a porfolio, but it’s a good reminder of progress."
  },
  {
    "objectID": "posts/all/vba-projects/index.html",
    "href": "posts/all/vba-projects/index.html",
    "title": "VBA Projects",
    "section": "",
    "text": "My initial data science projects were small scale VBA based automation projects.\nWhile I was a Benefits Data Analyst, the majority of my team’s work was in Excel, so I created tools using VBA to automate several processes. Additionally, these were added to my portfolio prior to a fuller understanding of Git and writing “proper” READMEs.\nThese models were built in a generalized scope prior to implementing at my workplace.\nGitHub Repository - Large Claimant Automation Tool\nGitHub Repository - Master Database Tool\nThese were truly the projects that started me down the road to data science, so I believe they are a staple of my portfolio. I was attempting to implement and automate both simple and complex tasks for an analyst team, but had the limitation of staying within Excel. Naturally, I taught myself VBA and begin to program solutions for my team. I was very proud of these!"
  },
  {
    "objectID": "posts/featured/snowbound/index.html",
    "href": "posts/featured/snowbound/index.html",
    "title": "Snowbound Analytics",
    "section": "",
    "text": "Applied the data science life cycle to data surrounding ski resorts; including weather, nearby businesses, and resort ratings across multiple parameters. An additional aspect featuring spatiotemporal analysis is also featured.\nGitHub Repository\nProject Website"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Downloadable Resume",
    "section": "",
    "text": "Feel free to download a copy of my resume!\n\nDownload Resume"
  }
]