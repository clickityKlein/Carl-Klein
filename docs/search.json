[
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Downloadable Resume",
    "section": "",
    "text": "Feel free to download a copy of my resume!\n\nDownload Resume"
  },
  {
    "objectID": "posts/featured/nlp-articles/index.html",
    "href": "posts/featured/nlp-articles/index.html",
    "title": "Natural Language Processing with Articles",
    "section": "",
    "text": "Created an NLP model which attempted to predict the publisher given a news article.\nGitHub Repository\nMedium Article\nThe goal of this project was to attempt to create an NLP model which could correctly identify the publisher given an article.\nAfter cleaning and exploring the data via statistical and visual methods, attempt to create a classification model which will accurately predict publisher if given a news article.\nWithout higher processing power, and a more equipt dataset, this will likely not be very accurate. Additionally, this only accounts for a small portion of the thousands of publishers and news networks, which means any article from outside the publishers in this dataset will result in an incorrect prediction.\nOverall, we were able to successfully clean and analyze a dataset surrounding our goal. After testing several models and parameters, there was a “best” model built. The f1-scores and accuracy were not ideal, however, the framework to continue to improve this model has been created!\nWhen it comes to potential improvements of this analysis and model, at the top of the list is obviously creating more accurate predictions. A starting point would be to perform a more robust parameter search. Given the proper hardware, it would be great to test the entire dataset (~150,000 articles) across not only each of the classifiers, but a grid search of parameters for each of the classifiers as well.\nUnfortunately, the instance of testing just the 3 max_depth parameters for the Random Forest Classifier on less than 10,000 rows of data took about 2 hours on the machine it was performed on. Maybe another machine is better equipt to take on the challenge!"
  },
  {
    "objectID": "posts/featured/bias-in-ml-models/index.html",
    "href": "posts/featured/bias-in-ml-models/index.html",
    "title": "Bias in Facial Classification ML Models",
    "section": "",
    "text": "An exploratory and statistical analysis on the biases prevalent in facial recognition machine learning models.\nGitHub Repository\nProject Website\nYouTube Video\nMy team, consisting of six members, performed a litany of analyses to test if there was bias in facial recognition machine learning models. We tested the DeepFace and FairFace algorithms against a large and diverse dataset. Performing both the classic performance measurements such as accuracy and f1-score, and categorical hypothesis testing (proportionality testing), we were able to find some instances of bias. Ironically, perhaps our biggest discovery was that categorial hypoothesis testing (proportionality testing) was not a strong indicator to identify issues and errors in machine learning models.\nSee the links above for the complete analysis and documentation."
  },
  {
    "objectID": "posts/all/rud-python-projects/index.html",
    "href": "posts/all/rud-python-projects/index.html",
    "title": "Rudimentary Python Projects",
    "section": "",
    "text": "As I was just starting to practice coding in python. Additionally, added to my portfolio prior to a fuller understanding of Git and writing “proper” READMEs.\nGitHub Repository - Expense Tracker Tool\nGitHub Repository - Linear Algebra Tools\nSome of the first functions written and files imported via Python were within these files. Normally, not something to showcase within a porfolio, but it’s a good reminder of progress."
  },
  {
    "objectID": "posts/all/flashcards-basic/index.html",
    "href": "posts/all/flashcards-basic/index.html",
    "title": "Basic Flashcards Application",
    "section": "",
    "text": "Application which allows users to build and combine decks of virtual flashcards. Supports mathematical notation and symbols.\nGitHub Repository\nActually, one of my first completed object oriented programming projects.\nThis project allows a user to create flashcards and decks of flashcards. It is meant to be a virtual replacement to physical flashcards, and features abilities such as deck combination and shuffling.\nIt was imperative to include the SymPy library, as this will personally be used heavily for mathematical concepts. On another personal note, further motivation for developing this into some sort of application was to swap mindless scrolling with learning during downtime."
  },
  {
    "objectID": "posts/all/disaster-response-dash/index.html",
    "href": "posts/all/disaster-response-dash/index.html",
    "title": "Disaster Response Dashboard",
    "section": "",
    "text": "Created a pipeline which classifies messages from various sources during an emergency. Additional step taken in deploying to Flask web application.\nGitHub Repository\nThe Disaster Response Pipeline classifies messages from various sources during an emergency. Rather than searching through potentially important key words, a model has been trained to categorize each message.\nIn the midst of an emergency, such as a natural disaster, thousands of messages are being sent. It’s important to be able to categorize these messages to optimize efforts and resources.\nBy utilizing natural language processing in a pipeline, a model was built to do just this.\nThe project used the following layout:\n\nETL Pipeline\nML Pipeline\nFlask Web App\n\nFuture Considerations:\n\nDue to time constraints and hardware limitations, a more complex model wasn’t tested or constructed. In the future it would be great to expand the parameter list while using GridSearchCV and try several different classifiers. XGBoost would be an ideal candidate.\nResearch for GridSearchCV also provided alternatives for GridSeachCV, itself. Among the list were RandomizedSearchCV and BayesSearch.\nIt should be mentioned that a significant portion of messages actually weren’t sorted into any of the 36 categories. This may initially feel like an error to train a model using these type of data points, however they are actually significant in the fact they are irrelevant. If a model is trained on only relevant messages, it will place a false priority on a irrelevant messages.\n\nUltimately, this project will benefit the community. In the event of a future disaster, millions of communications will be sent out, and response organizations will be at their full capacity. A model like this will help guide these organizations where to best use their resources."
  },
  {
    "objectID": "posts/all/bias-in-ml-models/index.html",
    "href": "posts/all/bias-in-ml-models/index.html",
    "title": "Bias in Facial Classification ML Models",
    "section": "",
    "text": "An exploratory and statistical analysis on the biases prevalent in facial recognition machine learning models.\nGitHub Repository\nProject Website\nYouTube Video\nMy team, consisting of six members, performed a litany of analyses to test if there was bias in facial recognition machine learning models. We tested the DeepFace and FairFace algorithms against a large and diverse dataset. Performing both the classic performance measurements such as accuracy and f1-score, and categorical hypothesis testing (proportionality testing), we were able to find some instances of bias. Ironically, perhaps our biggest discovery was that categorial hypoothesis testing (proportionality testing) was not a strong indicator to identify issues and errors in machine learning models.\nSee the links above for the complete analysis and documentation."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Carl Klein",
    "section": "",
    "text": "Welcome to my website! When I’m not in the mountains, I’m fine tuning my data science skills.\nI’m currently pursuing my master’s degree in data science (MSDS) at CU Boulder.\nPrior to this, I worked as a Benefits Data Analyst in the insurance industry. Initially, I had actuarial aspirations, inspired by my time at the University of Washington, where I studied mathematics (Bachelor of Arts in Mathematics, minor in Applied Mathematics).\nSmall scale automation projects during my time as a Benefits Data Analyst helped me start to develop a programming repertoire. I kept building the programming toolkit, eventually merging this with my passion for mathematics, and ultimately leading me down the data science pathway."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "The About Page"
  },
  {
    "objectID": "featured_projects.html",
    "href": "featured_projects.html",
    "title": "Featured Projects",
    "section": "",
    "text": "Bias in Facial Classification ML Models\n\n\n\n\n\n\npython\n\n\nr\n\n\nr-studio\n\n\nanalysis\n\n\nexploratory\n\n\nquarto\n\n\ndashboard\n\n\nshiny\n\n\nplotly\n\n\nhypothesis-testing\n\n\nmachine-learning\n\n\nstatistics\n\n\nlatex\n\n\nyoutube\n\n\n\n\n\n\n\n\n\nDec 18, 2023\n\n\nCarl Klein, Patrick Cooper, Bhavana Jonnalagadda, Piya (Leo) Ngamkam, Dhairya Veera\n\n\n\n\n\n\n\n\n\n\n\n\nNatural Language Processing with Articles\n\n\n\n\n\n\npython\n\n\nmedium\n\n\netl-pipeline\n\n\nanalysis\n\n\nexploratory\n\n\nnumpy\n\n\npandas\n\n\nmatplotlib\n\n\nnltk\n\n\nre\n\n\nsklearn\n\n\n\n\n\n\n\n\n\nJul 30, 2023\n\n\nCarl Klein\n\n\n\n\n\n\n\n\n\n\n\n\nDisaster Response Dashboard\n\n\n\n\n\n\npython\n\n\netl-pipeline\n\n\nmachine-learning-pipeline\n\n\nflask\n\n\njoblib\n\n\nnltk\n\n\nnumpy\n\n\npandas\n\n\nplotly\n\n\nsklearn\n\n\nsqlalchemy\n\n\nsql\n\n\ndashboard\n\n\n\n\n\n\n\n\n\nApr 27, 2023\n\n\nCarl Klein\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/all/basic-data-dash/index.html",
    "href": "posts/all/basic-data-dash/index.html",
    "title": "Basic Data Dashboard",
    "section": "",
    "text": "Template and instructions for deploying a dashboard which can be uploaded to a webpage using basic HTML, Flask, Plotly, and Bootstrap\nGitHub Repository\nThis template lays the foundation for creating a dashboard that can be uploaded to a webpage using basic HTML, Flask, Plotly, and Bootstrap. In the link above is a highly detailed guide in building and deploying a web application through Heroku. It covers Ubunto (linux) and Windows (GitBash) approaches."
  },
  {
    "objectID": "posts/all/covid-analysis/index.html",
    "href": "posts/all/covid-analysis/index.html",
    "title": "COVID Analysis",
    "section": "",
    "text": "Analyzed COVID-19 from several sources using R.\nGitHub Repository\nThe goal of this project was to manipulate COVID-19 and population data across multiple sources to solve desired inquiries. Local data from the United States as well as global data is analyzed.\nAmong the findings, there was one such visual I found compelling. The visual is of California broken down into counties showing a correlation of areas with higher concentrations of cases having higher concentrations of death.\n\n\n\nCalifornia Cases and Deaths"
  },
  {
    "objectID": "posts/all/emergency-calls/index.html",
    "href": "posts/all/emergency-calls/index.html",
    "title": "Emergency Calls",
    "section": "",
    "text": "Analysis of Seattle Police Department call center data.\nGitHub Repository\nMedium Article\nWith such a massive amount of well-maintained information available, this provided an opportunity to dive into what goes on when a police assistance call is placed. Specifically, we looked at:\n\nWhat times were the majority of the calls placed? Are there any surges by day of the month, or month of the year?\nDo the amount of calls change by Precinct, Sector or Beat?\nDoes the Priority of calls change by Precinct, Sector or Beat?\nThe Computer Aided Dispatch (CAD) system assigns Priority. Can we create a model which accurately predicts Priority?\n\nPlease see the GitHub repo and Medium articles for complete findings."
  },
  {
    "objectID": "posts/all/nlp-articles/index.html",
    "href": "posts/all/nlp-articles/index.html",
    "title": "Natural Language Processing with Articles",
    "section": "",
    "text": "Created an NLP model which attempted to predict the publisher given a news article.\nGitHub Repository\nMedium Article\nThe goal of this project was to attempt to create an NLP model which could correctly identify the publisher given an article.\nAfter cleaning and exploring the data via statistical and visual methods, attempt to create a classification model which will accurately predict publisher if given a news article.\nWithout higher processing power, and a more equipt dataset, this will likely not be very accurate. Additionally, this only accounts for a small portion of the thousands of publishers and news networks, which means any article from outside the publishers in this dataset will result in an incorrect prediction.\nOverall, we were able to successfully clean and analyze a dataset surrounding our goal. After testing several models and parameters, there was a “best” model built. The f1-scores and accuracy were not ideal, however, the framework to continue to improve this model has been created!\nWhen it comes to potential improvements of this analysis and model, at the top of the list is obviously creating more accurate predictions. A starting point would be to perform a more robust parameter search. Given the proper hardware, it would be great to test the entire dataset (~150,000 articles) across not only each of the classifiers, but a grid search of parameters for each of the classifiers as well.\nUnfortunately, the instance of testing just the 3 max_depth parameters for the Random Forest Classifier on less than 10,000 rows of data took about 2 hours on the machine it was performed on. Maybe another machine is better equipt to take on the challenge!"
  },
  {
    "objectID": "posts/all/vba-projects/index.html",
    "href": "posts/all/vba-projects/index.html",
    "title": "VBA Projects",
    "section": "",
    "text": "My inital projects were small scale automation projects while I was a Benefits Data Analyst. These were VBA based, as the majority of my team’s work was in Excel. Additionally, these were added to my portfolio prior to a fuller understanding of Git and writing “proper” READMEs.\nThese models were built in a generalized scope prior to implementing at my workplace.\nGitHub Repository - Large Claimant Automation Tool\nGitHub Repository - Master Database Tool\nThese were truly the projects that started me down the road to data science, so I believe they are a staple of my portfolio. I was attempting to implement and automate both simple and complex tasks for an analyst time, but had the limitation of staying within Excel. Naturally, I taught myself VBA and begin to program solutions for my team. I was very proud of these!"
  },
  {
    "objectID": "posts/featured/disaster-response-dash/index.html",
    "href": "posts/featured/disaster-response-dash/index.html",
    "title": "Disaster Response Dashboard",
    "section": "",
    "text": "Created a pipeline which classifies messages from various sources during an emergency. Additional step taken in deploying to Flask web application.\nGitHub Repository\nThe Disaster Response Pipeline classifies messages from various sources during an emergency. Rather than searching through potentially important key words, a model has been trained to categorize each message.\nIn the midst of an emergency, such as a natural disaster, thousands of messages are being sent. It’s important to be able to categorize these messages to optimize efforts and resources.\nBy utilizing natural language processing in a pipeline, a model was built to do just this.\nThe project used the following layout:\n\nETL Pipeline\nML Pipeline\nFlask Web App\n\nFuture Considerations:\n\nDue to time constraints and hardware limitations, a more complex model wasn’t tested or constructed. In the future it would be great to expand the parameter list while using GridSearchCV and try several different classifiers. XGBoost would be an ideal candidate.\nResearch for GridSearchCV also provided alternatives for GridSeachCV, itself. Among the list were RandomizedSearchCV and BayesSearch.\nIt should be mentioned that a significant portion of messages actually weren’t sorted into any of the 36 categories. This may initially feel like an error to train a model using these type of data points, however they are actually significant in the fact they are irrelevant. If a model is trained on only relevant messages, it will place a false priority on a irrelevant messages.\n\nUltimately, this project will benefit the community. In the event of a future disaster, millions of communications will be sent out, and response organizations will be at their full capacity. A model like this will help guide these organizations where to best use their resources."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "All Completed Projects",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nDescription\n\n\n\n\n\n\nDec 18, 2023\n\n\nBias in Facial Classification ML Models\n\n\n\n\n\n\n\nAug 23, 2023\n\n\nCOVID Analysis\n\n\n\n\n\n\n\nJul 30, 2023\n\n\nNatural Language Processing with Articles\n\n\n\n\n\n\n\nApr 27, 2023\n\n\nDisaster Response Dashboard\n\n\n\n\n\n\n\nMar 28, 2023\n\n\nBasic Data Dashboard\n\n\n\n\n\n\n\nJan 13, 2023\n\n\nBasic Flashcards Application\n\n\n\n\n\n\n\nNov 21, 2022\n\n\nEmergency Calls\n\n\n\n\n\n\n\nJun 16, 2022\n\n\nRudimentary Python Projects\n\n\n\n\n\n\n\nApr 24, 2022\n\n\nVBA Projects\n\n\n\n\n\n\n\n\nNo matching items"
  }
]